diff --git a/interact.py b/interact.py
index 9321587..919ffcb 100644
--- a/interact.py
+++ b/interact.py
@@ -32,7 +32,7 @@ else:
 
 state_dict = checkpoint['state_dict']
 opt = checkpoint['config']
-with open('SQuAD/meta.msgpack', 'rb') as f:
+with open('./meta.msgpack', 'rb') as f:
     meta = msgpack.load(f, encoding='utf8')
 embedding = torch.Tensor(meta['embedding'])
 opt['pretrained_words'] = True
diff --git a/prepro.py b/prepro.py
index 6e9197c..68cdc2a 100644
--- a/prepro.py
+++ b/prepro.py
@@ -1,5 +1,7 @@
 import re
 import json
+from os.path import join
+
 import spacy
 import msgpack
 import unicodedata
@@ -14,8 +16,8 @@ from drqa.utils import str2bool
 import logging
 
 
-def main():
-    args, log = setup()
+def main(dataset_path):
+    args, log = setup(dataset_path)
 
     train = flatten_json(args.trn_file, 'train')
     dev = flatten_json(args.dev_file, 'dev')
@@ -85,7 +87,7 @@ def main():
         'embedding': embeddings.tolist(),
         'wv_cased': args.wv_cased,
     }
-    with open('SQuAD/meta.msgpack', 'wb') as f:
+    with open('./meta.msgpack', 'wb') as f:
         msgpack.dump(meta, f)
     result = {
         'train': train,
@@ -95,7 +97,7 @@ def main():
     #        question_id, context, context_token_span, answer_start, answer_end
     # dev:   id, context_id, context_features, tag_id, ent_id,
     #        question_id, context, context_token_span, answer
-    with open('SQuAD/data.msgpack', 'wb') as f:
+    with open('./data.msgpack', 'wb') as f:
         msgpack.dump(result, f)
     if args.sample_size:
         sample = {
@@ -106,15 +108,15 @@ def main():
             msgpack.dump(sample, f)
     log.info('saved to disk.')
 
-def setup():
+def setup(dataset_path):
     parser = argparse.ArgumentParser(
         description='Preprocessing data files, about 10 minitues to run.'
     )
-    parser.add_argument('--trn_file', default='SQuAD/train-v1.1.json',
+    parser.add_argument('--trn_file', default=join(dataset_path, 'SQuAD/train-v1.1.json'),
                         help='path to train file.')
-    parser.add_argument('--dev_file', default='SQuAD/dev-v1.1.json',
+    parser.add_argument('--dev_file', default=join(dataset_path, 'SQuAD/dev-v1.1.json'),
                         help='path to dev file.')
-    parser.add_argument('--wv_file', default='glove/glove.840B.300d.txt',
+    parser.add_argument('--wv_file', default=join(dataset_path, 'glove/glove.840B.300d.txt'),
                         help='path to word vector file.')
     parser.add_argument('--wv_dim', type=int, default=300,
                         help='word vector dimension.')
@@ -130,7 +132,7 @@ def setup():
                         help='number of threads for preprocessing.')
     parser.add_argument('--batch_size', type=int, default=64,
                         help='batch size for multiprocess tokenizing and tagging.')
-    args = parser.parse_args()
+    args, _ = parser.parse_known_args()
 
     logging.basicConfig(format='%(asctime)s %(message)s', level=logging.DEBUG,
                         datefmt='%m/%d/%Y %I:%M:%S')
diff --git a/train.py b/train.py
index 0cdf157..608e66a 100644
--- a/train.py
+++ b/train.py
@@ -6,17 +6,20 @@ import random
 import string
 import logging
 import argparse
+from os.path import join
 from shutil import copyfile
 from datetime import datetime
 from collections import Counter
 import torch
 import msgpack
+import yaml
+
 from drqa.model import DocReaderModel
 from drqa.utils import str2bool
 
 
-def main():
-    args, log = setup()
+def main(output_path, config):
+    args, log = setup(output_path, config)
     log.info('[Program starts. Loading data...]')
     train, dev, dev_y, embedding, opt = load_data(vars(args))
     log.info(opt)
@@ -93,16 +96,17 @@ def main():
                 log.info('[new best model saved.]')
 
 
-def setup():
+def setup(output_path, config):
+
     parser = argparse.ArgumentParser(
         description='Train a Document Reader model.'
     )
     # system
     parser.add_argument('--log_per_updates', type=int, default=3,
                         help='log model loss per x updates (mini-batches).')
-    parser.add_argument('--data_file', default='SQuAD/data.msgpack',
+    parser.add_argument('--data_file', default='./data.msgpack',
                         help='path to preprocessed data file.')
-    parser.add_argument('--model_dir', default='models',
+    parser.add_argument('--model_dir', default=join(output_path, 'models'),
                         help='path to store saved models.')
     parser.add_argument('--save_last_only', action='store_true',
                         help='only save the final models.')
@@ -114,7 +118,7 @@ def setup():
                         const=True, default=torch.cuda.is_available(),
                         help='whether to use GPU acceleration.')
     # training
-    parser.add_argument('-e', '--epochs', type=int, default=40)
+    parser.add_argument('-e', '--epochs', type=int, default=config.get("epochs", 1))
     parser.add_argument('-bs', '--batch_size', type=int, default=32)
     parser.add_argument('-rs', '--resume', default='best_model.pt',
                         help='previous model file name (in `model_dir`). '
@@ -158,7 +162,7 @@ def setup():
     parser.add_argument('--rnn_type', default='lstm',
                         help='supported types: rnn, gru, lstm')
 
-    args = parser.parse_args()
+    args, _ = parser.parse_known_args()
 
     # set model dir
     model_dir = args.model_dir
@@ -210,7 +214,7 @@ def lr_decay(optimizer, lr_decay):
 
 
 def load_data(opt):
-    with open('SQuAD/meta.msgpack', 'rb') as f:
+    with open('./meta.msgpack', 'rb') as f:
         meta = msgpack.load(f, encoding='utf8')
     embedding = torch.Tensor(meta['embedding'])
     opt['pretrained_words'] = True
